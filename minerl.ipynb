{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import minerl\n",
    "import logging\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from importlib import reload\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ddpg_agent import Agent\n",
    "%matplotlib inline\n",
    "\n",
    "import ddpg_agent\n",
    "reload(ddpg_agent)\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "#pil_img = transforms.ToPILImage()(pov)\n",
    "#imshow(pil_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MineRLObtainDiamond-v0\") \n",
    "\n",
    "obs = env.reset()\n",
    "action = env.action_space.sample()\n",
    "\n",
    "mainhand = obs['equipped_items']['mainhand']\n",
    "inventory = obs['inventory']\n",
    "pov = obs['pov']\n",
    "\n",
    "agent_state_s = len(list(mainhand.values())) + len(list(inventory.values()))\n",
    "world_state_s = pov.shape\n",
    "action_s = len(list(action.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity()\n",
      "Identity()\n",
      "Identity()\n",
      "Identity()\n",
      "Embedding(5, 3)\n",
      "Embedding(8, 3)\n",
      "Identity()\n",
      "Identity()\n",
      "Identity()\n",
      "Embedding(8, 3)\n",
      "Embedding(3, 2)\n",
      "Embedding(7, 3)\n",
      "Identity()\n",
      "Identity()\n",
      "Identity()\n",
      "Identity()\n",
      "Identity()\n",
      "Identity()\n",
      "Identity()\n",
      "Embedding(5, 3)\n",
      "Embedding(8, 3)\n",
      "Identity()\n",
      "Identity()\n",
      "Identity()\n",
      "Embedding(8, 3)\n",
      "Embedding(3, 2)\n",
      "Embedding(7, 3)\n",
      "Identity()\n",
      "Identity()\n",
      "Identity()\n"
     ]
    }
   ],
   "source": [
    "import ddpg_agent\n",
    "reload(ddpg_agent)\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "\n",
    "agent = Agent(agent_state_size=21, world_state_size=(64, 64, 3), action_size=14, random_seed=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ddpg_agent\n",
    "reload(ddpg_agent)\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "def extract_data_from_dict(current_state, action, reward, next_state, done):\n",
    "\n",
    "    equipments = {\"none\":1, 'air':2, 'wooden_axe':3, 'wooden_pickaxe':4, \n",
    "              'stone_axe':5, 'stone_pickaxe':6, 'iron_axe':7, 'iron_pickaxe':8}\n",
    "\n",
    "\n",
    "    # current state\n",
    "    mainhand = current_state['equipped_items']['mainhand']\n",
    "    inventory = current_state['inventory']\n",
    "    pov = current_state['pov']\n",
    "\n",
    "\n",
    "    agent_state = []\n",
    "    agent_state.append(mainhand['damage'])\n",
    "    agent_state.append(mainhand['maxDamage'])\n",
    "    agent_state.append(mainhand['type'])\n",
    "    agent_state.append(inventory['coal'])\n",
    "    agent_state.append(inventory['cobblestone'])\n",
    "    agent_state.append(inventory['crafting_table'])\n",
    "    agent_state.append(inventory['dirt'])\n",
    "    agent_state.append(inventory['furnace'])\n",
    "    agent_state.append(inventory['iron_axe'])\n",
    "    agent_state.append(inventory['iron_ingot'])\n",
    "    agent_state.append(inventory['iron_ore'])\n",
    "    agent_state.append(inventory['iron_pickaxe'])\n",
    "    agent_state.append(inventory['log'])\n",
    "    agent_state.append(inventory['planks'])\n",
    "    agent_state.append(inventory['stick'])\n",
    "    agent_state.append(inventory['stone'])\n",
    "    agent_state.append(inventory['stone_axe'])\n",
    "    agent_state.append(inventory['stone_pickaxe'])\n",
    "    agent_state.append(inventory['torch'])\n",
    "    agent_state.append(inventory['wooden_axe'])\n",
    "    agent_state.append(inventory['wooden_pickaxe'])\n",
    "\n",
    "    #flat_list = [item for sublist in agent_state for item in sublist]\n",
    "    vertical_agent_state = [np.vstack(item) for item in agent_state]\n",
    "    concat_agent_state = np.concatenate(vertical_agent_state, axis=1)\n",
    "    \n",
    "    #[print(item.shape) for item in pov]\n",
    "    \n",
    "    swap_world_state = [np.swapaxes(item,0,2) for item in pov]\n",
    "    \n",
    "    #[print(item.shape) for item in swap_world_state]   \n",
    "    \n",
    "    vertical_world_state = np.stack(swap_world_state, axis=0)\n",
    "    #print(vertical_world_state.shape)\n",
    "    \n",
    "    #[print(item.shape) for item in vertical_world_state] \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # next_state\n",
    "    mainhand = next_state['equipped_items']['mainhand']\n",
    "    inventory = next_state['inventory']\n",
    "    pov = next_state['pov']\n",
    "\n",
    "\n",
    "    agent_state = []\n",
    "    agent_state.append(mainhand['damage'])\n",
    "    agent_state.append(mainhand['maxDamage'])\n",
    "    agent_state.append(mainhand['type'])\n",
    "    agent_state.append(inventory['coal'])\n",
    "    agent_state.append(inventory['cobblestone'])\n",
    "    agent_state.append(inventory['crafting_table'])\n",
    "    agent_state.append(inventory['dirt'])\n",
    "    agent_state.append(inventory['furnace'])\n",
    "    agent_state.append(inventory['iron_axe'])\n",
    "    agent_state.append(inventory['iron_ingot'])\n",
    "    agent_state.append(inventory['iron_ore'])\n",
    "    agent_state.append(inventory['iron_pickaxe'])\n",
    "    agent_state.append(inventory['log'])\n",
    "    agent_state.append(inventory['planks'])\n",
    "    agent_state.append(inventory['stick'])\n",
    "    agent_state.append(inventory['stone'])\n",
    "    agent_state.append(inventory['stone_axe'])\n",
    "    agent_state.append(inventory['stone_pickaxe'])\n",
    "    agent_state.append(inventory['torch'])\n",
    "    agent_state.append(inventory['wooden_axe'])\n",
    "    agent_state.append(inventory['wooden_pickaxe'])\n",
    "\n",
    "    #flat_list = [item for sublist in agent_state for item in sublist]\n",
    "    vertical_next_agent_state = [np.vstack(item) for item in agent_state]\n",
    "    concat_next_agent_state = np.concatenate(vertical_next_agent_state, axis=1)\n",
    "\n",
    "    swap_next_world_state = [np.swapaxes(item,0,2) for item in pov]\n",
    "    \n",
    "    #[print(item.shape) for item in swap_world_state]   \n",
    "    \n",
    "    vertical_next_world_state = np.stack(swap_next_world_state, axis=0)\n",
    "\n",
    "\n",
    "    \n",
    "    # get action list\n",
    "    \n",
    "    cam_0 = action[\"camera\"][:,0]\n",
    "    cam_1 = action[\"camera\"][:,1]\n",
    "    \n",
    "    \n",
    "    agent_actions = []\n",
    "    agent_actions.append(action[\"attack\"])\n",
    "    agent_actions.append(action[\"back\"])\n",
    "    agent_actions.append(cam_0)\n",
    "    agent_actions.append(cam_1)\n",
    "    agent_actions.append(action[\"craft\"])\n",
    "    agent_actions.append(action[\"equip\"])\n",
    "    agent_actions.append(action[\"forward\"])\n",
    "    agent_actions.append(action[\"jump\"])\n",
    "    agent_actions.append(action[\"left\"])\n",
    "    agent_actions.append(action[\"nearbyCraft\"])\n",
    "    agent_actions.append(action[\"nearbySmelt\"])\n",
    "    agent_actions.append(action[\"place\"])\n",
    "    agent_actions.append(action[\"right\"])\n",
    "    agent_actions.append(action[\"sneak\"])\n",
    "    agent_actions.append(action[\"sprint\"])\n",
    "    \n",
    "\n",
    "    \n",
    "    vertical_agent_actions = [np.vstack(item) for item in agent_actions]\n",
    "    concat_agent_actions = np.concatenate(vertical_agent_actions, axis=1)\n",
    "\n",
    "    experiences = zip(concat_agent_state, vertical_world_state, concat_agent_actions, reward, concat_next_agent_state, vertical_next_world_state, done)\n",
    "\n",
    "    experiences = np.array(list(experiences))\n",
    "    return experiences\n",
    "    \n",
    "    \n",
    "data = minerl.data.make(\n",
    "    'MineRLObtainDiamond-v0',\n",
    "    data_dir='/Users/desin/minerl/minerl_data/')\n",
    "\n",
    "agent = Agent(agent_state_size=21, world_state_size=(64, 64, 3), action_size=14, random_seed=0)\n",
    "\n",
    "\n",
    "# Iterate through a single epoch gathering sequences of at most 32 steps\n",
    "i=0\n",
    "for current_state, action, reward, next_state, done \\\n",
    "    in data.sarsd_iter(\n",
    "        num_epochs=1, max_sequence_len=32):\n",
    "\n",
    "        i = i+1\n",
    "        if i%3==0:\n",
    "            i=0\n",
    "            continue\n",
    "        done = np.delete(done, -1)\n",
    "        experiences = extract_data_from_dict(current_state, action, reward, next_state, done)\n",
    "        agent.learn_from_players(experiences)\n",
    "        \n",
    "        action_1, action_raw = agent.act(mainhand, inventory, pov)\n",
    "        #action_1 = env.action_space.sample()\n",
    "        #print(action_1)\n",
    "        obs, reward_1, done_1, _ = env.step(action_1)\n",
    "        #print(reward_1)\n",
    "        mainhand = obs['equipped_items']['mainhand']\n",
    "        inventory = obs['inventory']\n",
    "        pov = obs['pov']\n",
    "        env.render()\n",
    "\n",
    "        # ... do something with the data.\n",
    "        #print(\"At the end of trajectories the length\"\n",
    "        #      \"can be < max_sequence_len\", len(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_action_space', '_get_all_valid_recordings', '_is_blacklisted', '_load_data_pyfunc', '_observation_space', '_roundrobin', 'action_space', 'data_dir', 'environment', 'get_trajectory_names', 'load_data', 'map_to_dict', 'number_of_workers', 'observation_space', 'processing_pool', 'sarsd_iter', 'seed', 'seq_iter', 'size_to_dequeue', 'worker_batch_size']\n",
      "<bound method DataPipeline.seq_iter of <minerl.data.data_pipeline.DataPipeline object at 0x13a0e7c50>>\n",
      "Dict(equipped_items:Dict(mainhand:Dict(damage:Box(), maxDamage:Box(), type:Enum(none,air,wooden_axe,wooden_pickaxe,stone_axe,stone_pickaxe,iron_axe,iron_pickaxe,other))), inventory:Dict(coal:Box(), cobblestone:Box(), crafting_table:Box(), dirt:Box(), furnace:Box(), iron_axe:Box(), iron_ingot:Box(), iron_ore:Box(), iron_pickaxe:Box(), log:Box(), planks:Box(), stick:Box(), stone:Box(), stone_axe:Box(), stone_pickaxe:Box(), torch:Box(), wooden_axe:Box(), wooden_pickaxe:Box()), pov:Box(64, 64, 3))\n"
     ]
    }
   ],
   "source": [
    "print(dir(data))\n",
    "print(data.seq_iter)\n",
    "print(data.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ddpg_agent\n",
    "reload(ddpg_agent)\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "print(agent_state_s)\n",
    "print(world_state_s)\n",
    "print(action_s)\n",
    "\n",
    "agent = Agent(agent_state_size=agent_state_s, world_state_size=world_state_s, action_size=action_s, random_seed=0)\n",
    "\n",
    "\n",
    "def ddpg(n_episodes=5000, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        obs = env.reset()\n",
    "        done = False        \n",
    "        agent.reset()\n",
    "        \n",
    "        mainhand = obs['equipped_items']['mainhand']\n",
    "        inventory = obs['inventory']\n",
    "        pov = obs['pov']\n",
    "\n",
    "        scores = []  \n",
    "        actions = []\n",
    "        \n",
    "        t = 0\n",
    "        while True:\n",
    "            \n",
    "            action, action_raw = agent.act(mainhand, inventory, pov)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            n_mainhand = obs['equipped_items']['mainhand']\n",
    "            n_inventory = obs['inventory']\n",
    "            n_pov = obs['pov']\n",
    "            scores.append(reward)\n",
    "            if reward != 0.0:\n",
    "                print(reward)\n",
    "            \n",
    "            action_flat = agent.flatten_action(action_raw)\n",
    "\n",
    "            agent.step(mainhand, inventory, pov, action_flat, reward, n_mainhand, n_inventory, n_pov, done)\n",
    "\n",
    "            mainhand = n_mainhand \n",
    "            inventory = n_inventory \n",
    "            pov = n_pov\n",
    "            \n",
    "            env.render()\n",
    "            if np.any(done):                                  # exit loop if episode finished\n",
    "                break \n",
    "                \n",
    "        scores_avg = np.mean(scores)\n",
    "        scores_deque.append(scores_avg)\n",
    "        scores_all.append(scores_avg)\n",
    "        #print(scores_agents)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:2.3f}\\tScore: {:2.3f}       '.format(i_episode, np.mean(scores_deque), scores_avg), end=\"\")\n",
    "        if i_episode % 200 == 0:\n",
    "            print(agent.actor_scheduler.get_lr())\n",
    "            print('\\nEpisode {}\\tAverage Score: {:2.3f}\\tScore: {:2.3f}\\tBalls Over: {}       '.format(i_episode, np.mean(scores_deque), scores_avg, balls_over), end=\"\")\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor_{}.pth'.format(i_episode))\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic_{}.pth'.format(i_episode))   \n",
    "            \n",
    "    return scores_all\n",
    "\n",
    "#scores = ddpg(n_episodes=10, max_t=130)\n",
    "\n",
    "\n",
    "scores = ddpg()\n",
    "print('scores len  {}'.format(len(scores)))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "fig.savefig('AverageScore.png')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
